{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55dacc2c-5749-4862-813f-814f7fab4548",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andreagentilini/Desktop/temp/myenv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "# import kagglehub\n",
    "import zipfile\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import FileLink, display\n",
    "%matplotlib inline\n",
    "\n",
    "# --- Device ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# --- Data Loading ---\n",
    "# Impostiamo i parametri per il caricamento\n",
    "num_slices = 1000      # numero di slice da caricare\n",
    "num_playlists = 1000   # numero di playlist per ogni slice\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11f6e2bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1jj_ApW79I2n2n4skXmXrNWVWroOlaEuM\n",
      "From (redirected): https://drive.google.com/uc?id=1jj_ApW79I2n2n4skXmXrNWVWroOlaEuM&confirm=t&uuid=0e705b24-7b8d-4f58-969c-012c5fcc47c1\n",
      "To: /Users/andreagentilini/Downloads/data_zipped.zip\n",
      "100%|██████████████████████████████████████| 5.75G/5.75G [01:25<00:00, 67.2MB/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Estrai il contenuto nella directory specificata\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m zipfile\u001b[38;5;241m.\u001b[39mZipFile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata_zipped.zip\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m zip_ref:\n\u001b[0;32m----> 8\u001b[0m     \u001b[43mzip_ref\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextractall\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m)\u001b[49m  \n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/zipfile/__init__.py:1775\u001b[0m, in \u001b[0;36mZipFile.extractall\u001b[0;34m(self, path, members, pwd)\u001b[0m\n\u001b[1;32m   1772\u001b[0m     path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mfspath(path)\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m zipinfo \u001b[38;5;129;01min\u001b[39;00m members:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_extract_member\u001b[49m\u001b[43m(\u001b[49m\u001b[43mzipinfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpwd\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/zipfile/__init__.py:1837\u001b[0m, in \u001b[0;36mZipFile._extract_member\u001b[0;34m(self, member, targetpath, pwd)\u001b[0m\n\u001b[1;32m   1833\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m targetpath\n\u001b[1;32m   1835\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopen(member, pwd\u001b[38;5;241m=\u001b[39mpwd) \u001b[38;5;28;01mas\u001b[39;00m source, \\\n\u001b[1;32m   1836\u001b[0m      \u001b[38;5;28mopen\u001b[39m(targetpath, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m target:\n\u001b[0;32m-> 1837\u001b[0m     \u001b[43mshutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopyfileobj\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1839\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m targetpath\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/shutil.py:203\u001b[0m, in \u001b[0;36mcopyfileobj\u001b[0;34m(fsrc, fdst, length)\u001b[0m\n\u001b[1;32m    201\u001b[0m fsrc_read \u001b[38;5;241m=\u001b[39m fsrc\u001b[38;5;241m.\u001b[39mread\n\u001b[1;32m    202\u001b[0m fdst_write \u001b[38;5;241m=\u001b[39m fdst\u001b[38;5;241m.\u001b[39mwrite\n\u001b[0;32m--> 203\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m buf \u001b[38;5;241m:=\u001b[39m \u001b[43mfsrc_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlength\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    204\u001b[0m     fdst_write(buf)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/zipfile/__init__.py:1012\u001b[0m, in \u001b[0;36mZipExtFile.read\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m   1010\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_offset \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   1011\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m n \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eof:\n\u001b[0;32m-> 1012\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1013\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(data):\n\u001b[1;32m   1014\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_readbuffer \u001b[38;5;241m=\u001b[39m data\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/zipfile/__init__.py:1088\u001b[0m, in \u001b[0;36mZipExtFile._read1\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m   1086\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compress_type \u001b[38;5;241m==\u001b[39m ZIP_DEFLATED:\n\u001b[1;32m   1087\u001b[0m     n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(n, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mMIN_READ_SIZE)\n\u001b[0;32m-> 1088\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_decompressor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecompress\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1089\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eof \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decompressor\u001b[38;5;241m.\u001b[39meof \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m   1090\u001b[0m                  \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compress_left \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m   1091\u001b[0m                  \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decompressor\u001b[38;5;241m.\u001b[39munconsumed_tail)\n\u001b[1;32m   1092\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eof:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Specifica un percorso personalizzato (modifica secondo le tue esigenze)\n",
    "data_dir = \"data/\"\n",
    "# Scarica il file ZIP\n",
    "!gdown \"1jj_ApW79I2n2n4skXmXrNWVWroOlaEuM\" -O data_zipped.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cff9da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estrai il contenuto nella directory specificata\n",
    "with zipfile.ZipFile(\"data_zipped.zip\", 'r') as zip_ref:\n",
    "    zip_ref.extractall(data_dir)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1dc8915",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join(data_dir, 'data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b3a1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista dei file estratti nella cartella specificata\n",
    "slices = sorted(os.listdir(data_dir))[:num_slices]\n",
    "\n",
    "print(f\"Caricati {len(slices)} file dalla cartella '{data_dir}':\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599b1b2b-4deb-4e85-a4a1-de1d4410b590",
   "metadata": {},
   "outputs": [],
   "source": [
    "playlists = []\n",
    "for slice_file in slices:\n",
    "    with open(os.path.join(data_dir, slice_file), \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    # Aggiungiamo solo le prime num_playlists playlist per ogni slice\n",
    "    playlists.extend(data['playlists'][:num_playlists])\n",
    "\n",
    "# --- Preprocessing: estrazione della feature \"track_name\" ---\n",
    "playlists_tracks = [\n",
    "    [track['track_name'] for track in playlist['tracks']]\n",
    "    for playlist in playlists\n",
    "]\n",
    "\n",
    "# --- Costruzione del vocabolario ---\n",
    "unique_tracks = {track for playlist in playlists_tracks for track in playlist}\n",
    "if '.' in unique_tracks:\n",
    "    unique_tracks.remove('.')\n",
    "# Il token '.' (stop/start) avrà indice 0\n",
    "stoi = {track: i+1 for i, track in enumerate(sorted(unique_tracks))}\n",
    "stoi['.'] = 0\n",
    "itos = {i: track for track, i in stoi.items()}\n",
    "vocab_size = len(itos)\n",
    "print(\"Vocab size:\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fefb591-3f80-4307-9d30-6165175a7229",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Creazione del dataset per il language model ---\n",
    "block_size = 5  # lunghezza del contesto\n",
    "X_data, Y_data = [], []\n",
    "for playlist in playlists_tracks:\n",
    "    context = [0] * block_size  # inizializziamo il contesto con il token di stop\n",
    "    # Aggiungiamo il token di stop alla fine della playlist\n",
    "    for track in playlist + ['.']:\n",
    "        ix = stoi[track]\n",
    "        X_data.append(context.copy())\n",
    "        Y_data.append(ix)\n",
    "        # Aggiorniamo il contesto: shift a sinistra e aggiungiamo l'indice corrente\n",
    "        context = context[1:] + [ix]\n",
    "\n",
    "X = torch.tensor(X_data, dtype=torch.long)\n",
    "Y = torch.tensor(Y_data, dtype=torch.long)\n",
    "print(\"Dataset shape:\", X.shape, Y.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a18a391",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Suddivisione train/val/test ---\n",
    "indices = torch.randperm(X.shape[0])\n",
    "X = X[indices]\n",
    "Y = Y[indices]\n",
    "n_total = X.shape[0]\n",
    "n_train = int(0.8 * n_total)\n",
    "n_val   = int(0.1 * n_total)\n",
    "\n",
    "Xtr, Ytr = X[:n_train], Y[:n_train]\n",
    "Xval, Yval = X[n_train:n_train+n_val], Y[n_train:n_train+n_val]\n",
    "Xte, Yte = X[n_train+n_val:], Y[n_train+n_val:]\n",
    "print(\"Training samples:\", Xtr.shape[0])\n",
    "print(\"Validation samples:\", Xval.shape[0])\n",
    "print(\"Test samples:\", Xte.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6767d05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Definizione del modello ---\n",
    "class PlaylistModel(nn.Module):\n",
    "    def __init__(self, vocab_size, n_embd, block_size, n_hidden):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, n_embd)\n",
    "        self.block_size = block_size\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(n_embd * block_size, n_hidden, bias=False),\n",
    "            nn.BatchNorm1d(n_hidden),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(n_hidden, n_hidden, bias=False),\n",
    "            nn.BatchNorm1d(n_hidden),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(n_hidden, n_hidden, bias=False),\n",
    "            nn.BatchNorm1d(n_hidden),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(n_hidden, n_hidden, bias=False),\n",
    "            nn.BatchNorm1d(n_hidden),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(n_hidden, n_hidden, bias=False),\n",
    "            nn.BatchNorm1d(n_hidden),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(n_hidden, vocab_size, bias=False),\n",
    "            nn.BatchNorm1d(vocab_size)\n",
    "        )\n",
    "        # Riduciamo la scala del layer BN finale\n",
    "        final_bn = self.mlp[-1]\n",
    "        final_bn.weight.data.mul_(0.1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        emb = self.embedding(x)                   # -> (batch_size, block_size, n_embd)\n",
    "        emb = emb.view(emb.size(0), -1)             # -> (batch_size, block_size * n_embd)\n",
    "        logits = self.mlp(emb)                      # -> (batch_size, vocab_size)\n",
    "        return logits\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c146e431",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_embd = 100\n",
    "n_hidden = 100\n",
    "model = PlaylistModel(vocab_size, n_embd, block_size, n_hidden).to(device)\n",
    "print(\"Numero di parametri:\", sum(p.numel() for p in model.parameters()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb21b8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Ottimizzatore e DataLoader ---\n",
    "batch_size = 64\n",
    "train_dataset = TensorDataset(Xtr, Ytr)\n",
    "val_dataset   = TensorDataset(Xval, Yval)\n",
    "test_dataset  = TensorDataset(Xte, Yte)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "numb_step_change = 1500\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_loss(loader):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_samples = 0\n",
    "    for xb, yb in loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        logits = model(xb)\n",
    "        loss = F.cross_entropy(logits, yb, reduction='sum')\n",
    "        total_loss += loss.item()\n",
    "        total_samples += xb.size(0)\n",
    "    model.train()\n",
    "    return total_loss / total_samples\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f9ec12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Training ---\n",
    "max_steps = 2000\n",
    "log_interval = 100\n",
    "step = 0\n",
    "train_loss_history = []\n",
    "val_loss_history = []\n",
    "steps_list = []\n",
    "\n",
    "model.train()\n",
    "\n",
    "while step < max_steps:\n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(xb)\n",
    "        loss = F.cross_entropy(logits, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss_history.append(loss.item())\n",
    "        step += 1\n",
    "\n",
    "        if step == numb_step_change:\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = 0.01\n",
    "\n",
    "        if step % log_interval == 0:\n",
    "            print('ok')\n",
    "            val_loss = evaluate_loss(val_loader)\n",
    "            steps_list.append(step)\n",
    "            val_loss_history.append(val_loss)\n",
    "            print(f'{step:7d}/{max_steps:7d}: Train loss = {loss.item():.4f}, Val loss = {val_loss:.4f}')\n",
    "\n",
    "        if step >= max_steps:\n",
    "            break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f2c86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Plot di train e validation loss ---\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_loss_history, label=\"Train Loss\", alpha=0.6)\n",
    "plt.plot(steps_list, val_loss_history, 'ro-', label=\"Validation Loss\", markersize=5)\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"Cross Entropy Loss\")\n",
    "plt.title(\"Training e Validation Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# --- Valutazione finale sui dataset ---\n",
    "print(\"Train Loss:\", evaluate_loss(train_loader))\n",
    "print(\"Val Loss:\", evaluate_loss(val_loader))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83c9191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Calcolo delle metriche sul test set ---\n",
    "k_values = [1, 2, 3, 5]\n",
    "model.eval()\n",
    "\n",
    "all_logits = []\n",
    "all_targets = []\n",
    "with torch.no_grad():\n",
    "    for xb, yb in test_loader:\n",
    "        xb = xb.to(device)\n",
    "        logits = model(xb)\n",
    "        all_logits.append(logits.cpu())\n",
    "        all_targets.append(yb.cpu())\n",
    "all_logits = torch.cat(all_logits, dim=0)\n",
    "all_targets = torch.cat(all_targets, dim=0)\n",
    "\n",
    "test_loss = F.cross_entropy(all_logits, all_targets).item()\n",
    "print(\"Test Loss:\", test_loss)\n",
    "\n",
    "probs = F.softmax(all_logits, dim=1)\n",
    "ranking = torch.argsort(probs, dim=1, descending=True)\n",
    "\n",
    "precision_at_k = {k: 0.0 for k in k_values}\n",
    "recall_at_k = {k: 0.0 for k in k_values}\n",
    "N = all_targets.shape[0]\n",
    "\n",
    "for k in k_values:\n",
    "    topk = ranking[:, :k]\n",
    "    correct = (topk == all_targets.unsqueeze(1)).any(dim=1).float()\n",
    "    precision_at_k[k] = (correct / k).mean().item()\n",
    "    recall_at_k[k] = correct.mean().item()\n",
    "\n",
    "for k in k_values:\n",
    "    print(f\"Precision@{k}: {precision_at_k[k]:.4f}, Recall@{k}: {recall_at_k[k]:.4f}\")\n",
    "\n",
    "true_logits = all_logits[torch.arange(N), all_targets].unsqueeze(1)\n",
    "ranks = (all_logits >= true_logits).sum(dim=1).float()\n",
    "mrr = (1.0 / ranks).mean().item()\n",
    "print(f\"MRR: {mrr:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5aa047",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "implementare hyperparameter tuning su n\\_embd, n\\_hidden e il batch size, \n",
    "dato che questi parametri controllano la capacità del modello e l’efficienza del training. \n",
    "Inizia definendo uno spazio di ricerca ragionevole (ad es. n\\_embd \\in \\{50, 100, 200, 300\\}, n\\_hidden \\in \\{100, 200, 300, 400\\}, batch size \\in \\{32, 64, 128\\}) \n",
    "e valuta l’adozione di tecniche come la grid search e random search. \n",
    "Utilizza un set di validazione affidabile e monitora sia il loss che le metriche specifiche del task (come Precision@k e MRR) per guidare le tue scelte.\n",
    "\"\"\"\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11568c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Salvataggio del modello trainato ---\n",
    "model_path = \"playlist_model.pth\"\n",
    "torch.save(model.state_dict(), model_path)\n",
    "print(\"Modello salvato in:\", model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc4dc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Creazione di un link per il download del file ---\n",
    "# Soluzione 1: usando IPython.display.FileLink\n",
    "print(\"Clicca sul link sottostante per scaricare il modello:\")\n",
    "display(FileLink(model_path))\n",
    "\n",
    "# Soluzione 2 (alternativa): se fossi in Google Colab, puoi usare:\n",
    "# from google.colab import files\n",
    "# files.download(model_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
