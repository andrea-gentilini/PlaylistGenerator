{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Rr8nGXQgGhiV",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title import dependencies\n",
        "import os\n",
        "import re\n",
        "import gc\n",
        "import json\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from scipy.sparse import coo_matrix\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from scipy.sparse.linalg import svds\n",
        "\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Downloading and Analysis"
      ],
      "metadata": {
        "id": "-pQMKS8xwgQH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Download dataset from Kaggle\n",
        "import kagglehub\n",
        "\n",
        "path = \"/root/.cache/kagglehub/datasets/himanshuwagh/spotify-million/versions/1\"\n",
        "if not os.path.exists(path):\n",
        "  # Download latest version\n",
        "  path = kagglehub.dataset_download(\"himanshuwagh/spotify-million\")\n",
        "\n",
        "# contiene le slices del dataset: 1000 slice das 1000 playlist ciascuna\n",
        "data: str = os.path.join(path, \"data\")"
      ],
      "metadata": {
        "id": "7qxXTE6xGska"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title shuffle slices in a list and pick from them\n",
        "shuffled_slices = np.array(os.listdir(data))\n",
        "np.random.shuffle(shuffled_slices)"
      ],
      "metadata": {
        "id": "KZl7WOvAOQaj"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shuffled_slices[:3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cYvh7DdkeDjg",
        "outputId": "ef5ee331-9f18-45c3-cf10-e404ca6bdd04"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['mpd.slice.756000-756999.json', 'mpd.slice.949000-949999.json',\n",
              "       'mpd.slice.953000-953999.json'], dtype='<U28')"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SVD Approach"
      ],
      "metadata": {
        "id": "RDESyr5dxGX7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Handling"
      ],
      "metadata": {
        "id": "5BqlPLiGHGQ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "million_df = pd.DataFrame()\n",
        "num_training_files = 500\n",
        "\n",
        "# Create an empty list to hold all rows as dictionaries\n",
        "data_list = []\n",
        "uri_to_info = dict()\n",
        "playlist_to_follower = dict()\n",
        "\n",
        "#for i, filename in tqdm(enumerate(sorted(os.listdir(data), key=extract_starting_number)[:num_training_files]), desc=\"Processing Slices\"):\n",
        "for i, filename in tqdm(enumerate(shuffled_slices[:num_training_files]), desc=\"Processing Slices\", total = num_training_files):\n",
        "    if filename.startswith(\"mpd.slice.\") and filename.endswith(\".json\"):\n",
        "        filepath = os.path.join(data, filename)\n",
        "\n",
        "        with open(filepath, \"r\", encoding=\"utf-8\") as jsonfile:\n",
        "            cur_slice = json.load(jsonfile)\n",
        "\n",
        "        # for playlist in tqdm(cur_slice[\"playlists\"], desc=\"Processing playlist...\"):\n",
        "        for playlist in cur_slice[\"playlists\"]:\n",
        "            playlist_id = playlist[\"pid\"]\n",
        "            if not playlist_id in playlist_to_follower:\n",
        "              playlist_to_follower[playlist_id] = playlist[\"num_followers\"]\n",
        "\n",
        "            # Collect data for the playlist\n",
        "            for track in playlist[\"tracks\"]:\n",
        "                data_list.append({\n",
        "                    \"playlist\": playlist_id,\n",
        "                    \"track\": track[\"track_uri\"][14:]  # remove 'spotify:track:'\n",
        "                })\n",
        "                if track[\"track_uri\"][14:] not in uri_to_info:\n",
        "                  uri_to_info[track[\"track_uri\"][14:]] = (track[\"artist_name\"], track[\"track_name\"])\n",
        "\n",
        "    # update every 30 files for speedup\n",
        "    if i%30 == 0:\n",
        "        new_data = pd.DataFrame(data_list)\n",
        "        data_list.clear()\n",
        "        million_df = pd.concat([million_df, new_data], ignore_index=True)\n",
        "\n",
        "# Convert the list of dictionaries into a DataFrame in one go\n",
        "# dumb_dataset = pd.DataFrame(data_list)\n",
        "new_data = pd.DataFrame(data_list)\n",
        "data_list = []\n",
        "million_df = pd.concat([million_df, new_data], ignore_index=True)\n",
        "\n",
        "million_df[\"playlist\"] = million_df[\"playlist\"].astype(\"int32\")\n",
        "\n",
        "max_followers = max(playlist_to_follower.values())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ix4fSvZPHIjN",
        "outputId": "51a04174-4c9f-43ca-8d53-c1f5d80c5a70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Slices:  33%|███▎      | 166/500 [01:47<02:29,  2.23it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7QpCHcYCnVQ-"
      },
      "outputs": [],
      "source": [
        "million_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x8m7kd91dICA"
      },
      "outputs": [],
      "source": [
        "million_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YHOwOJbOdI0j"
      },
      "outputs": [],
      "source": [
        "million_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y8SuQT-iS5yS"
      },
      "outputs": [],
      "source": [
        "uri_to_appeareance = dict()\n",
        "\n",
        "# Count how many playlists each track appears in\n",
        "track_frequency = million_df.groupby(\"track\")[\"playlist\"].nunique()\n",
        "total_songs = len(track_frequency)\n",
        "\n",
        "# Convert to a DataFrame for easier handling\n",
        "track_frequency_df = track_frequency.reset_index().rename(columns={\"playlist\": \"playlist_count\"})\n",
        "\n",
        "# Total number of playlists\n",
        "# total_playlists = million_df[\"playlist_id\"].nunique()\n",
        "total_playlists = 1000*num_training_files\n",
        "\n",
        "# threshold\n",
        "threshold = int(total_playlists * 0.00005)\n",
        "\n",
        "# Filter tracks that appear in at least the threshold number of playlists\n",
        "popular_tracks = track_frequency_df[track_frequency_df[\"playlist_count\"] >= threshold]\n",
        "partial = len(popular_tracks)\n",
        "\n",
        "for uri, a in zip(popular_tracks[\"track\"], popular_tracks[\"playlist_count\"]):\n",
        "  if uri not in uri_to_appeareance:\n",
        "    uri_to_appeareance[uri] = a\n",
        "\n",
        "\n",
        "# Extract popular track IDs\n",
        "popular_track_ids = popular_tracks[\"track\"].tolist()\n",
        "\n",
        "# Filter the original dataset\n",
        "filtered_df = million_df[million_df[\"track\"].isin(popular_track_ids)]\n",
        "\n",
        "max_appeareance = max(uri_to_appeareance.values())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DxbI3CjQnNll"
      },
      "outputs": [],
      "source": [
        "print(\"Minumum number of playlists: \", threshold)\n",
        "ratio = partial / total_songs\n",
        "print(\"Total songs: \", total_songs)\n",
        "print(f\"Songs that appear in at least {threshold} playlists: \", partial)\n",
        "print(f\"Ratio: {ratio:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Songs with most appearences in playlists: \")\n",
        "for uri, playlist in zip(track_frequency_df.sort_values(\"playlist_count\", ascending=False).head()[\"track\"], track_frequency_df.sort_values(\"playlist_count\", ascending=False).head()[\"playlist_count\"]):\n",
        "  artist, name = uri_to_info[uri]\n",
        "  print(f\"{artist} - {name} (Appears in {playlist} playlists, {playlist/total_playlists:.2f}% of total playlist in the training set)\")"
      ],
      "metadata": {
        "id": "r5DdHxCUmShH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SeVcOGmhdj3_"
      },
      "outputs": [],
      "source": [
        "filtered_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lUFqR4g2pAcb"
      },
      "outputs": [],
      "source": [
        "filtered_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s8DGS_xXnc7e"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "# Make an explicit copy of filtered_df\n",
        "filtered_df = filtered_df.copy()\n",
        "\n",
        "# Map playlist_id and track_id to numerical indices\n",
        "playlist_id_to_idx = {id: idx for idx, id in enumerate(filtered_df[\"playlist\"].unique())}\n",
        "track_uri_to_idx = {uri: idx for idx, uri in enumerate(filtered_df[\"track\"].unique())}\n",
        "\n",
        "filtered_df[\"playlist_idx\"] = filtered_df[\"playlist\"].map(playlist_id_to_idx)\n",
        "filtered_df[\"track_idx\"] = filtered_df[\"track\"].map(track_uri_to_idx)\n",
        "\n",
        "# Create COO matrix\n",
        "rows = filtered_df[\"playlist_idx\"]\n",
        "cols = filtered_df[\"track_idx\"]\n",
        "data_list = np.ones(len(filtered_df))  # All entries are 1 since a track belongs to a playlist\n",
        "\n",
        "coo_rating_matrix = coo_matrix((data_list, (rows, cols)), shape=(len(playlist_id_to_idx), len(track_uri_to_idx)))\n",
        "print(coo_rating_matrix.shape)  # Output: (485376, 18857)\n",
        "\"\"\"\n",
        "\n",
        "# Make an explicit copy of filtered_df\n",
        "filtered_df = filtered_df.copy()\n",
        "\n",
        "# Map playlist_id and track_id to numerical indices\n",
        "playlist_id_to_idx = {id: idx for idx, id in enumerate(filtered_df[\"playlist\"].unique())}\n",
        "playlist_idx_to_id = {idx: id for id, idx in playlist_id_to_idx.items()}\n",
        "track_uri_to_idx = {uri: idx for idx, uri in enumerate(filtered_df[\"track\"].unique())}\n",
        "track_idx_to_uri = {idx: uri for uri, idx in track_uri_to_idx.items()}\n",
        "\n",
        "\n",
        "filtered_df[\"playlist_idx\"] = filtered_df[\"playlist\"].map(playlist_id_to_idx)\n",
        "filtered_df[\"track_idx\"] = filtered_df[\"track\"].map(track_uri_to_idx)\n",
        "\n",
        "# Create COO matrix\n",
        "rows = filtered_df[\"playlist_idx\"]\n",
        "cols = filtered_df[\"track_idx\"]\n",
        "\n",
        "coo_rating_matrix = np.zeros((len(playlist_id_to_idx), len(track_uri_to_idx)))\n",
        "for playlist_idx, track_idx in zip(rows, cols):\n",
        "  alpha, beta = 10, 5\n",
        "  song_popularity = uri_to_appeareance[track_idx_to_uri[track_idx]] / max_appeareance\n",
        "  playlist_popularity = playlist_to_follower[playlist_idx_to_id[playlist_idx]] / max_followers\n",
        "  coo_rating_matrix[playlist_idx, track_idx] = alpha*song_popularity + beta*playlist_popularity\n",
        "\n",
        "print(coo_rating_matrix.shape)  # Output: (485376, 18857)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# per rendere il codice della funzione di valutazione come quello sopra\n",
        "num_tracks = coo_rating_matrix.shape[1]\n",
        "tracks = set(track_uri_to_idx.keys())"
      ],
      "metadata": {
        "id": "TWW_TB_ySr1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "json.dump(uri_to_info, open(\"uri_to_info.json\", \"w\"))\n",
        "del(uri_to_info)\n",
        "del(million_df, track_frequency_df, filtered_df)\n",
        "del(rows, cols, data_list, new_data)\n",
        "del(popular_tracks, popular_track_ids)\n",
        "del(playlist_id_to_idx, track_frequency)\n",
        "\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "mLB0fidEHut9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gc.collect()"
      ],
      "metadata": {
        "id": "mHO4kakPH24z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "wJW3mVilLoJJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ScipySVD():\n",
        "  def __init__(self, n_components, **kwargs):\n",
        "    self.n_components = n_components\n",
        "    self.kwargs = kwargs\n",
        "\n",
        "\n",
        "  def fit(self, X):\n",
        "    _, _, components_ = svds(X, self.n_components, **self.kwargs)\n",
        "    self.components_ = components_\n",
        "\n",
        "\n",
        "  def transform(self ,X):\n",
        "    return X @ self.components_.T"
      ],
      "metadata": {
        "id": "TXin71eMIPvV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "svd_model = ScipySVD(600, random_state=42)\n",
        "\n",
        "svd_model.fit(coo_rating_matrix)"
      ],
      "metadata": {
        "id": "_5zwJS1cKbrf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Validation"
      ],
      "metadata": {
        "id": "Hg5WR4cdLOQm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title metrics definition\n",
        "def precision_at_k(predicted_matrix, ground_truth_matrix, input_matrix, k):\n",
        "    \"\"\"\n",
        "    Calculate precision at k while excluding items that are already in the input matrix (playlist).\n",
        "\n",
        "    Args:\n",
        "    - predicted_matrix (np.ndarray): Matrix of predicted scores for each song in the playlist.\n",
        "    - ground_truth_matrix (np.ndarray): Ground truth matrix with binary values indicating relevant songs.\n",
        "    - input_matrix (np.ndarray): Matrix representing songs already in the playlist (binary).\n",
        "    - k (int): The number of top items to consider.\n",
        "\n",
        "    Returns:\n",
        "    - float: The average precision at k, excluding already present songs.\n",
        "    \"\"\"\n",
        "    # Ensure the matrices have the same shape\n",
        "    assert predicted_matrix.shape == ground_truth_matrix.shape == input_matrix.shape, \\\n",
        "        \"Shape mismatch between predicted_matrix, ground_truth_matrix, and input_matrix.\"\n",
        "\n",
        "    # Create a mask for already existing items (input matrix)\n",
        "    mask = input_matrix > 0  # 1 indicates the item is already in the playlist\n",
        "\n",
        "    # Mask the predicted scores for already existing items by setting them to -inf\n",
        "    masked_predictions = np.where(mask, -np.inf, predicted_matrix)\n",
        "\n",
        "    # Use argsort to get the indices of the top k predictions after masking\n",
        "    top_k_indices = np.argsort(masked_predictions, axis=1)[:, ::-1][:, :k]\n",
        "\n",
        "    # Extract relevant items in ground truth corresponding to the top k predictions\n",
        "    relevant_items = ground_truth_matrix[np.arange(ground_truth_matrix.shape[0])[:, None], top_k_indices]\n",
        "\n",
        "    # Calculate precision as the number of relevant items divided by k\n",
        "    precision_scores = np.sum(relevant_items, axis=1) / k\n",
        "\n",
        "    # Return the average precision\n",
        "    return np.mean(precision_scores)\n",
        "\n",
        "\n",
        "\n",
        "def recall_at_k(predicted_matrix, ground_truth_matrix, input_matrix, k):\n",
        "    # Mask the predictions where the input matrix has 1s (already in the playlist)\n",
        "    mask = input_matrix == 1\n",
        "    masked_predictions = np.where(mask, -np.inf, predicted_matrix)\n",
        "\n",
        "    # Get the indices of the top k predictions for each row after masking\n",
        "    top_k_indices = np.argsort(masked_predictions, axis=1)[:, -k:][:, ::-1]\n",
        "\n",
        "    # Gather the relevant items in ground truth corresponding to top k predictions\n",
        "    relevant_items = ground_truth_matrix[np.arange(ground_truth_matrix.shape[0])[:, None], top_k_indices]\n",
        "\n",
        "    # Calculate the recall for each playlist\n",
        "    total_relevant = np.sum(ground_truth_matrix, axis=1)  # Total relevant items per playlist\n",
        "\n",
        "    # Avoid division by zero: mask rows with no relevant items\n",
        "    recall_scores = np.sum(relevant_items, axis=1) / np.maximum(total_relevant, 1)\n",
        "\n",
        "    # Return the mean recall, ignoring rows with no relevant items\n",
        "    return np.mean(recall_scores[total_relevant > 0])\n",
        "\n",
        "\n",
        "\n",
        "def mean_reciprocal_rank(predicted_matrix, ground_truth_matrix, input_matrix):\n",
        "    reciprocal_ranks = []\n",
        "\n",
        "    # Iterate over each playlist (row in the matrix)\n",
        "    for pred_row, true_row, input_row in zip(predicted_matrix, ground_truth_matrix, input_matrix):\n",
        "        # Mask the predictions where the input matrix has 1s (already in the playlist)\n",
        "        mask = input_row == 1\n",
        "        masked_predictions = np.where(mask, -np.inf, pred_row)\n",
        "\n",
        "        # Get the indices sorted by predicted scores in descending order\n",
        "        sorted_indices = np.argsort(masked_predictions)[::-1]\n",
        "\n",
        "        # Find the rank of the first relevant item\n",
        "        found_relevant = False\n",
        "        for rank, index in enumerate(sorted_indices, start=1):\n",
        "            if true_row[index] == 1:  # If the item is relevant in the ground truth\n",
        "                reciprocal_ranks.append(1 / rank)\n",
        "                found_relevant = True\n",
        "                break\n",
        "\n",
        "        # If no relevant items were found, append 0\n",
        "        if not found_relevant:\n",
        "            reciprocal_ranks.append(0)\n",
        "\n",
        "    # Return the mean of the reciprocal ranks\n",
        "    return np.mean(reciprocal_ranks)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "X_DF2313G_f3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title evaluating functions definition\n",
        "def evaluate_model_k_tracks_removed_df(model, k, num_valid_files=10):\n",
        "  \"\"\"\n",
        "  evaluate model processing a slice of playlists, 200 playlist at time to avoid\n",
        "  colab cpu overflow\n",
        "  \"\"\"\n",
        "\n",
        "  precision_at_10 = np.zeros(num_valid_files)\n",
        "  precision_at_5 = np.zeros(num_valid_files)\n",
        "  precision_at_2 = np.zeros(num_valid_files)\n",
        "  precision_at_1 = np.zeros(num_valid_files)\n",
        "\n",
        "  recall_at_10 = np.zeros(num_valid_files)\n",
        "  recall_at_5 = np.zeros(num_valid_files)\n",
        "  recall_at_2 = np.zeros(num_valid_files)\n",
        "  recall_at_1 = np.zeros(num_valid_files)\n",
        "\n",
        "  mrr = np.zeros(num_valid_files)\n",
        "\n",
        "  for file_idx, filename in enumerate(shuffled_slices[num_training_files:num_training_files+num_valid_files]):\n",
        "    correct_playlists = np.zeros((1000, num_tracks))\n",
        "    p_counter = -1\n",
        "    if filename.startswith(\"mpd.slice.\") and filename.endswith(\".json\"):\n",
        "      filepath = os.path.join(data, filename)\n",
        "\n",
        "      with open(filepath, \"r\", encoding=\"utf-8\") as jsonfile:\n",
        "        cur_slice = json.load(jsonfile)\n",
        "\n",
        "      for playlist in cur_slice[\"playlists\"]:\n",
        "        p_counter += 1\n",
        "\n",
        "        for track in playlist[\"tracks\"]:\n",
        "          track_uri = track[\"track_uri\"][14:]\n",
        "\n",
        "          if track_uri in tracks:\n",
        "            t_idx = track_uri_to_idx[track_uri]\n",
        "\n",
        "            correct_playlists[p_counter, t_idx] = 1\n",
        "\n",
        "\n",
        "    incomplete_playlists = np.copy(correct_playlists)\n",
        "\n",
        "    # Turn exactly k ones to zeros per row\n",
        "    for row in incomplete_playlists:\n",
        "      # Get the indices of `1`s in the current row\n",
        "      one_indices = np.where(row == 1)[0]\n",
        "\n",
        "      if len(one_indices) >= k:\n",
        "        indices_to_zero = np.random.choice(one_indices, size=k, replace=False)\n",
        "        row[indices_to_zero] = 0\n",
        "\n",
        "    n_iter = 5\n",
        "\n",
        "    cur_precision_at_10 = [0 for _ in range(n_iter)]\n",
        "    cur_precision_at_5 = [0 for _ in range(n_iter)]\n",
        "    cur_precision_at_2 = [0 for _ in range(n_iter)]\n",
        "    cur_precision_at_1 = [0 for _ in range(n_iter)]\n",
        "\n",
        "    cur_recall_at_10 = [0 for _ in range(n_iter)]\n",
        "    cur_recall_at_5 = [0 for _ in range(n_iter)]\n",
        "    cur_recall_at_2 = [0 for _ in range(n_iter)]\n",
        "    cur_recall_at_1 = [0 for _ in range(n_iter)]\n",
        "\n",
        "    cur_mrr = [0 for _ in range(n_iter)]\n",
        "\n",
        "    size_batch = 1000 // n_iter\n",
        "\n",
        "    for iter in range(n_iter):\n",
        "      input_matrix_iter = incomplete_playlists[size_batch*iter:size_batch*(iter+1), :]\n",
        "      P_new = model.transform(input_matrix_iter)\n",
        "\n",
        "      # Predici la matrice ricostruita per le nuove playlist\n",
        "      predicted_matrix = np.dot(P_new, model.components_)\n",
        "\n",
        "      ground_truth_matrix_iter = correct_playlists[size_batch*iter:size_batch*(iter+1), :]\n",
        "\n",
        "      cur_precision_at_10[iter] = precision_at_k(predicted_matrix, ground_truth_matrix_iter, input_matrix_iter, 10)\n",
        "      cur_precision_at_5[iter] = precision_at_k(predicted_matrix, ground_truth_matrix_iter, input_matrix_iter, 5)\n",
        "      cur_precision_at_2[iter] = precision_at_k(predicted_matrix, ground_truth_matrix_iter, input_matrix_iter, 2)\n",
        "      cur_precision_at_1[iter] = precision_at_k(predicted_matrix, ground_truth_matrix_iter, input_matrix_iter, 1)\n",
        "\n",
        "      cur_recall_at_10[iter] = recall_at_k(predicted_matrix, ground_truth_matrix_iter, input_matrix_iter, 10)\n",
        "      cur_recall_at_5[iter] = recall_at_k(predicted_matrix, ground_truth_matrix_iter, input_matrix_iter, 5)\n",
        "      cur_recall_at_2[iter] = recall_at_k(predicted_matrix, ground_truth_matrix_iter, input_matrix_iter, 2)\n",
        "      cur_recall_at_1[iter] = recall_at_k(predicted_matrix, ground_truth_matrix_iter, input_matrix_iter, 1)\n",
        "\n",
        "      cur_mrr[iter] = mean_reciprocal_rank(predicted_matrix, ground_truth_matrix_iter, input_matrix_iter)\n",
        "\n",
        "    precision_at_10[file_idx] = np.mean(cur_precision_at_10)\n",
        "    precision_at_5[file_idx] = np.mean(cur_precision_at_5)\n",
        "    precision_at_2[file_idx] = np.mean(cur_precision_at_2)\n",
        "    precision_at_1[file_idx] = np.mean(cur_precision_at_1)\n",
        "    recall_at_10[file_idx] = np.mean(cur_recall_at_10)\n",
        "    recall_at_5[file_idx] = np.mean(cur_recall_at_5)\n",
        "    recall_at_2[file_idx] = np.mean(cur_recall_at_2)\n",
        "    recall_at_1[file_idx] = np.mean(cur_recall_at_1)\n",
        "    mrr[file_idx] = np.mean(cur_mrr)\n",
        "\n",
        "  print(\"Precision@10 = \",np.mean(precision_at_10))\n",
        "  print(\"Precision@5 = \",np.mean(precision_at_5))\n",
        "  print(\"Precision@2 = \",np.mean(precision_at_2))\n",
        "  print(\"Precision@1 = \",np.mean(precision_at_1))\n",
        "\n",
        "  print(\"Recall@10 = \",np.mean(recall_at_10))\n",
        "  print(\"Recall@5 = \",np.mean(recall_at_5))\n",
        "  print(\"Recall@2 = \",np.mean(recall_at_2))\n",
        "  print(\"Recall@1 = \",np.mean(recall_at_1))\n",
        "\n",
        "  print(\"MRR = \", np.mean(mrr))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def evaluate_model_k_tracks_per_playlist(model, k, num_valid_files=10):\n",
        "  \"\"\"\n",
        "  evaluate model processing a slice of playlists, 200 playlist at time to avoid\n",
        "  colab cpu overflow\n",
        "  \"\"\"\n",
        "\n",
        "  # num_valid_files = 1000 - num_training_files\n",
        "  precision_at_10 = np.zeros(num_valid_files)\n",
        "  precision_at_5 = np.zeros(num_valid_files)\n",
        "  precision_at_2 = np.zeros(num_valid_files)\n",
        "  precision_at_1 = np.zeros(num_valid_files)\n",
        "\n",
        "  recall_at_10 = np.zeros(num_valid_files)\n",
        "  recall_at_5 = np.zeros(num_valid_files)\n",
        "  recall_at_2 = np.zeros(num_valid_files)\n",
        "  recall_at_1 = np.zeros(num_valid_files)\n",
        "\n",
        "  mrr = np.zeros(num_valid_files)\n",
        "\n",
        "  for file_idx, filename in enumerate(shuffled_slices[num_training_files:num_training_files+num_valid_files]):\n",
        "    correct_playlists = np.zeros((1000, num_tracks))\n",
        "    p_counter = -1\n",
        "    if filename.startswith(\"mpd.slice.\") and filename.endswith(\".json\"):\n",
        "      filepath = os.path.join(data, filename)\n",
        "\n",
        "      with open(filepath, \"r\", encoding=\"utf-8\") as jsonfile:\n",
        "        cur_slice = json.load(jsonfile)\n",
        "\n",
        "      for playlist in cur_slice[\"playlists\"]:\n",
        "        p_counter += 1\n",
        "\n",
        "        for track in playlist[\"tracks\"]:\n",
        "          track_uri = track[\"track_uri\"][14:]\n",
        "\n",
        "          if track_uri in tracks:\n",
        "            t_idx = track_uri_to_idx[track_uri]\n",
        "\n",
        "            correct_playlists[p_counter, t_idx] = 1\n",
        "\n",
        "\n",
        "    incomplete_playlists = np.copy(correct_playlists)\n",
        "\n",
        "    for row in incomplete_playlists:\n",
        "      one_indexes = np.where(row == 1)[0]\n",
        "\n",
        "      if len(one_indexes) >= k:\n",
        "        indices_to_zero = np.random.choice(one_indexes, size=(len(one_indexes)-k), replace=False)\n",
        "        row[indices_to_zero] = 0\n",
        "\n",
        "    n_iter = 5\n",
        "\n",
        "    cur_precision_at_10 = [0 for _ in range(n_iter)]\n",
        "    cur_precision_at_5 = [0 for _ in range(n_iter)]\n",
        "    cur_precision_at_2 = [0 for _ in range(n_iter)]\n",
        "    cur_precision_at_1 = [0 for _ in range(n_iter)]\n",
        "\n",
        "    cur_recall_at_10 = [0 for _ in range(n_iter)]\n",
        "    cur_recall_at_5 = [0 for _ in range(n_iter)]\n",
        "    cur_recall_at_2 = [0 for _ in range(n_iter)]\n",
        "    cur_recall_at_1 = [0 for _ in range(n_iter)]\n",
        "\n",
        "    cur_mrr = [0 for _ in range(n_iter)]\n",
        "\n",
        "    size_batch = 1000 // n_iter\n",
        "\n",
        "    for iter in range(n_iter):\n",
        "      input_matrix_iter = incomplete_playlists[size_batch*iter:size_batch*(iter+1), :]\n",
        "      P_new = model.transform(input_matrix_iter)\n",
        "\n",
        "      # Predici la matrice ricostruita per le nuove playlist\n",
        "      predicted_matrix = np.dot(P_new, model.components_)\n",
        "\n",
        "      ground_truth_matrix_iter = correct_playlists[size_batch*iter:size_batch*(iter+1), :]\n",
        "\n",
        "      cur_precision_at_10[iter] = precision_at_k(predicted_matrix, ground_truth_matrix_iter, input_matrix_iter, 10)\n",
        "      cur_precision_at_5[iter] = precision_at_k(predicted_matrix, ground_truth_matrix_iter, input_matrix_iter, 5)\n",
        "      cur_precision_at_2[iter] = precision_at_k(predicted_matrix, ground_truth_matrix_iter, input_matrix_iter, 2)\n",
        "      cur_precision_at_1[iter] = precision_at_k(predicted_matrix, ground_truth_matrix_iter, input_matrix_iter, 1)\n",
        "\n",
        "      cur_recall_at_10[iter] = recall_at_k(predicted_matrix, ground_truth_matrix_iter, input_matrix_iter, 10)\n",
        "      cur_recall_at_5[iter] = recall_at_k(predicted_matrix, ground_truth_matrix_iter, input_matrix_iter, 5)\n",
        "      cur_recall_at_2[iter] = recall_at_k(predicted_matrix, ground_truth_matrix_iter, input_matrix_iter, 2)\n",
        "      cur_recall_at_1[iter] = recall_at_k(predicted_matrix, ground_truth_matrix_iter, input_matrix_iter, 1)\n",
        "\n",
        "      cur_mrr[iter] = mean_reciprocal_rank(predicted_matrix, ground_truth_matrix_iter, input_matrix_iter)\n",
        "\n",
        "    precision_at_10[file_idx] = np.mean(cur_precision_at_10)\n",
        "    precision_at_5[file_idx] = np.mean(cur_precision_at_5)\n",
        "    precision_at_2[file_idx] = np.mean(cur_precision_at_2)\n",
        "    precision_at_1[file_idx] = np.mean(cur_precision_at_1)\n",
        "    recall_at_10[file_idx] = np.mean(cur_recall_at_10)\n",
        "    recall_at_5[file_idx] = np.mean(cur_recall_at_5)\n",
        "    recall_at_2[file_idx] = np.mean(cur_recall_at_2)\n",
        "    recall_at_1[file_idx] = np.mean(cur_recall_at_1)\n",
        "    mrr[file_idx] = np.mean(cur_mrr)\n",
        "\n",
        "  print(\"Precision@10 = \",np.mean(precision_at_10))\n",
        "  print(\"Precision@5 = \",np.mean(precision_at_5))\n",
        "  print(\"Precision@2 = \",np.mean(precision_at_2))\n",
        "  print(\"Precision@1 = \",np.mean(precision_at_1))\n",
        "\n",
        "  print(\"Recall@10 = \",np.mean(recall_at_10))\n",
        "  print(\"Recall@5 = \",np.mean(recall_at_5))\n",
        "  print(\"Recall@2 = \",np.mean(recall_at_2))\n",
        "  print(\"Recall@1 = \",np.mean(recall_at_1))\n",
        "\n",
        "  print(\"MRR = \", np.mean(mrr))"
      ],
      "metadata": {
        "id": "EApnpa-aLSX2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "K1 = [0, 2, 5, 15, 30]\n",
        "K2 = [2, 5, 15, 30]\n",
        "\n",
        "for k in K1:\n",
        "  print(f\"\\n Validation Metrics Removing {k} songs:\")\n",
        "  evaluate_model_k_tracks_removed_df(svd_model, k, 2)\n",
        "  print()\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "for k in K2:\n",
        "  print(f\"\\n Validation Metrics Keeping {k} songs per playlist:\")\n",
        "  evaluate_model_k_tracks_per_playlist(svd_model, k, 2)\n",
        "  print()\n"
      ],
      "metadata": {
        "id": "4SuVi9BI_zQh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Language Model Approach"
      ],
      "metadata": {
        "id": "o8buF6l80anu"
      }
    }
  ]
}